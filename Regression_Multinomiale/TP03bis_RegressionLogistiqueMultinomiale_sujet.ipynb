{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID-TP03bis. Régression logistique Multinomiale\n",
    "\n",
    "**INTRODUCTION**\n",
    "\n",
    "Nous avons implémenté le cas d'une seule classe (binaire : oui ou non). Pour appliquer un classement sur plusieurs classes, on peut entrainner $L$ modèles de régression logistique (où $L$ est le nombre des classes). Dans ce cas, nos résultats (Y) doivent encodée en 0 et 1. Pour un modèle $M_i$ d'une classe $C_i$, la sortie $Y$ doit avoir 1 si $C_i$, 0 si une autre classe. (One-to-rest classification)\n",
    "\n",
    "Une autre approche (celle qu'on va implémenter) est d'encoder la sortie en utilisant OneHot encoder. Pour $L$ classes et un échantillon donnée, on va avoir $L$ sorties (une ayant 1 et les autres 0). Pour un dataset avec $M$ échantillons, $N$ caractéristiques et $L$ classes, on va avoir les dimensions suivantes : \n",
    "\n",
    "- $X [M, N]$\n",
    "- $Y [M, L]$\n",
    "- $\\theta [N, L]$\n",
    "\n",
    "Cette dernière approche s'appelle maximum entropy (MaxEnt). C'est une généralisation de la régresion logistique binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Dans cette partie, on va essayer d'implémenter la régression multinomiale afin de mieux comprendre son principe. \n",
    "Pour ce faire, on va utiliser la bibliothèque **numpy** qui est utile dans les calcules surtout matricielles.\n",
    "\n",
    "### I.1. Combinaison linéaire\n",
    "\n",
    "On combine linéairement les $N$  caractéristiques de la même façon que la régression linéaire binaire. \n",
    "La seule différence est que nous avons plus de classes, donc le nombre des paramètres va être multiplié par le nombre des classes.\n",
    "La somme pondérée d'une classe $c$ est calculée selon la formule : \n",
    "\n",
    "$$Z_c = zfn_c(X, \\theta) = \\sum\\limits_{j=0}^{N} \\theta_{(c, j)} X_j | X_0 = 1 $$\n",
    "\n",
    "La forme matricielle de $Z$ sera : \n",
    "$$Z = zfn(X, \\theta) = X \\cdot \\theta$$\n",
    "\n",
    "- $X[M, N]$ : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $\\theta[N, L]$ : une matrice de N lignes (caractéristiques, y compris le biais) et L colonnes (classes). \n",
    "- $Z[M, L]$ : une matrice de M lignes (échantillons) et L colonnes (classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implémenter la fonction de combinaison linéaire \n",
    "def zfn(X: np.ndarray, Theta: np.ndarray) -> np.ndarray: \n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0. , 0. , 0. ],\n",
    "#        [0.5, 0.1, 0.6],\n",
    "#        [0.2, 0.3, 0. ],\n",
    "#        [0.7, 0.4, 0.6]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_tn = np.array([[0., 0.], \n",
    "                 [1., 0.], \n",
    "                 [0., 1.], \n",
    "                 [1., 1.]]) # 4 échntillons, 2 caractéristiques\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "zfn(X_tn, Theta_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Calcul des probabilités\n",
    "\n",
    "Les valeurs combinées sont transformées à des probabilités en utilisant la fonction softmax. \n",
    "La fonction softmax nous assure que la somme des probabilités des classes égale à 1.\n",
    "Cette fonction prend les combinaisons linéaires $Z[M, L]$ et calcule les probabilités $P[M, L] comme suite : \n",
    "\n",
    "$$softmax(Z)=\\frac{e^Z}{\\sum\\limits_{k=1}^{L} e^{Z_k}}$$\n",
    "\n",
    "- $M$ nombre des échantillons\n",
    "- $N$ nombre des caractéristiques\n",
    "- $L$ nombre des classes\n",
    "- La somme des probabilités de chaque ligne doit être 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter la fonction softmax\n",
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0.33333333, 0.33333333, 0.33333333],\n",
    "#       [0.36029662, 0.24151404, 0.39818934],\n",
    "#       [0.34200877, 0.37797814, 0.28001309],\n",
    "#       [0.37797814, 0.28001309, 0.34200877]])\n",
    "#---------------------------------------------------------------------\n",
    "Z_tn = np.array([[0. , 0. , 0. ],\n",
    "                 [0.5, 0.1, 0.6],\n",
    "                 [0.2, 0.3, 0. ],\n",
    "                 [0.7, 0.4, 0.6]])\n",
    "softmax(Z_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Prédiction \n",
    "\n",
    "Etant donnée les probabilités des classes pour chaque échantillon, on doit choisir la classe avec le max de probabilité.\n",
    "\n",
    "$$\n",
    "\\hat{C}^{(i)}_j = \n",
    "\\begin{cases}\n",
    "1 & si & H^{(i)}_j \\ge \\max P^{(i)}\\\\\n",
    "0 & sinon & \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $H[M, L]$ probabilités où chaque ligne est yn échantillon et chaque collone est une classe\n",
    "- $\\hat{C}[M, L]$ prédictions où chaque ligne est yn échantillon et chaque collone est une classe. $\\hat{C}^{(i)}_j \\in \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter la fonction de prédiction \n",
    "def cn(H: np.ndarray) -> np.ndarray: \n",
    "    res = np.zeros(H.shape)\n",
    "    # ...\n",
    "    return res\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 0, 0],\n",
    "#        [0, 0, 1],\n",
    "#        [0, 1, 0],\n",
    "#        [1, 0, 0]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "             [0.36029662, 0.24151404, 0.39818934],\n",
    "             [0.34200877, 0.37797814, 0.28001309],\n",
    "             [0.37797814, 0.28001309, 0.34200877]])\n",
    "cn(H_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Calcul du coût \n",
    "\n",
    "On réfère aux probabilités trouvées par la fonction softmax comme $H$, où $H_c$ est la probabilité d'une classe $c$.\n",
    "Etant donné un échantillon $X^{(i)}$, son coût est calculé comme : \n",
    "\n",
    "$$ cout(H^{(i)}, Y^{(i)}) = - \\sum\\limits_{c=1}^{L} Y^{(i)}_c \\log(H^{(i)}_c)$$\n",
    "\n",
    "Le coût total est la moyenne des coût de tous les échantillons\n",
    "\n",
    "$$J(H, Y) = \\frac{1}{M} \\sum\\limits_{i=1}^{M} cout(H^{(i)}, Y^{(i)})$$\n",
    "\n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter la fonction du coût\n",
    "def jn(H: np.ndarray, Y: np.ndarray, eps: float=1e-8) -> float: \n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 1.1913194196637922\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "\n",
    "Y_tn = np.array([[1, 0, 0], \n",
    "                 [0, 1, 0], \n",
    "                 [0, 0, 1], \n",
    "                 [1, 0, 0]])\n",
    "\n",
    "jn(H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Calcul des gradients\n",
    "\n",
    "La taille des gradients est la même que celle des paramètres $\\theta[N, L]$. \n",
    "\n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X^{(i)}_{j} $$\n",
    "\n",
    "Sa forme matricielle sera \n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} X^\\top \\cdot (H-Y) $$\n",
    "\n",
    "- $X[M, N]$ : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)\n",
    "- $\\frac{\\partial J}{\\theta}[N, L]$ : une matrice de L lignes (classes) et N colonnes (caractéristiques, y compris le biais). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter le gradient de la fonction softmax\n",
    "def dJn(X: np.ndarray, H: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    return  None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[-0.06543131, -0.11961822,  0.18504953],\n",
    "#        [-0.07000327,  0.16449781, -0.09449454]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], \n",
    "                 [1., 0.], \n",
    "                 [0., 1.], \n",
    "                 [1., 1.]])\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "\n",
    "Y_tn = np.array([[1, 0, 0], \n",
    "                 [0, 1, 0], \n",
    "                 [0, 0, 1], \n",
    "                 [1, 0, 0]])\n",
    "\n",
    "dJn(X_tn, H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.6. Regrouper les fonctions ensemble \n",
    "\n",
    "**Rien à programmer ici**\n",
    "\n",
    "Pour bien gérer l'entrainnement et la prédiction, on rassemble les fonctions que vous avez implémenté dans une seul classe. L'intérêt : \n",
    "- Si on applique la normalisation durant l'entrainnement, on doit l'appliquer aussi durant la prédiction. En plus, on doit utiliser les mêmes paramètres (moyenne et écart-type)\n",
    "- On utilise les thétas optimales lors de la prédicition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.6.1. Descente des gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def des_grad(X, Y, Theta, max_iter=200, alpha=0.1):\n",
    "    \n",
    "    couts = []\n",
    "    \n",
    "    Theta = Theta.copy() # pour ne pas modifier Theta original\n",
    "    \n",
    "    for i in range(max_iter): # Ici, la seule condition d'arrêt est le nombre des itérations\n",
    "        H = softmax(zfn(X, Theta))\n",
    "        couts.append(jn(H, Y))\n",
    "        Theta = Theta - alpha * dJn(X, H, Y)\n",
    "    \n",
    "    return Theta, couts\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array([[ 0.84298097,  1.57919742, -1.22217839],\n",
    "#         [ 0.60036187, -1.45101777,  1.3506559 ]]),\n",
    "#  0.5977646723518713)\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], \n",
    "                 [1., 0.], \n",
    "                 [0., 1.], \n",
    "                 [1., 1.]]) # deux variables logiques\n",
    "\n",
    "Y_tn = np.array([[1, 0, 0], \n",
    "                 [0, 1, 0], \n",
    "                 [0, 0, 1], \n",
    "                 [1, 0, 0]]) # égale, sup, inf, égale\n",
    "\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "\n",
    "theta_n, couts_n = des_grad(X_tn, Y_tn, Theta_tn)\n",
    "\n",
    "theta_n, couts_n[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.6.2. Fonctions supplimentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliser(X, mean=None, std=None): \n",
    "    if (mean is None) or (std is None): \n",
    "        mean = np.mean(X, axis=0)\n",
    "        std = np.std(X, axis=0)\n",
    "    X_norm = np.where(std==0, X, (X - mean)/std)\n",
    "    return X_norm, mean, std\n",
    "\n",
    "def preparer(X, norm=True, const=True, mean=None, std=None): \n",
    "    X_pre = X.copy()\n",
    "    if norm: \n",
    "        X_pre, mean, std = normaliser(X_pre,mean=mean, std=std)\n",
    "    if const:\n",
    "        X_pre = np.append(np.ones((X_pre.shape[0],1)), X_pre ,axis=1)\n",
    "    return X_pre, mean, std\n",
    "\n",
    "def generer_zeros_1(nbr):\n",
    "    return np.zeros(nbr)\n",
    "\n",
    "def generer_uns_1(nbr):\n",
    "    return np.ones(nbr)\n",
    "\n",
    "def generer_aleatoire_1(nbr):\n",
    "    return np.random.rand(nbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.6.3. Classe pour MaxEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt(object):\n",
    "    \n",
    "    def __init__(self, norm=True, const=True): \n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "    \n",
    "    def entrainer(self, X, Y, max_iter=100, alpha=.01, adagrad=False): \n",
    "        X_pre, self.mean, self.std = preparer(X, norm=self.norm, const=self.const)\n",
    "        Theta = np.zeros((X_pre.shape[1], Y.shape[1])) # Theta[N, L]\n",
    "        self.Theta, self.couts = des_grad(X_pre, Y, Theta, max_iter=max_iter, alpha=alpha)\n",
    "        \n",
    "        \n",
    "    # La prédiction\n",
    "    # si prob=True elle rend un vecteur de probabilités\n",
    "    # sinon elle rend une vecteur de 1 et 0\n",
    "    def predire(self, X, prob=True):\n",
    "        X_pre, self.mean, self.std = preparer(X, norm=self.norm, const=self.const, mean=self.mean, std=self.std)\n",
    "        H = softmax(zfn(X_pre, self.Theta))\n",
    "        if prob:\n",
    "            return H\n",
    "        return cn(H)\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 0, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 0, 1]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], \n",
    "                 [1., 0.], \n",
    "                 [0., 1.], \n",
    "                 [1., 1.]]) # deux variables logiques\n",
    "\n",
    "Y_tn = np.array([[1, 0, 0], \n",
    "                 [0, 1, 0], \n",
    "                 [0, 0, 1], \n",
    "                 [1, 0, 0]]) # égale, sup, inf, égale\n",
    "\n",
    "X_testn = np.array([[2., 2.], \n",
    "                    [1., 0.], \n",
    "                    [1., -1.], \n",
    "                    [2., 5.]])\n",
    "\n",
    "maxent = MaxEnt()\n",
    "maxent.entrainer(X_tn, Y_tn)\n",
    "maxent.predire(X_testn, prob=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "On va utiliser [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) pour classer des fleurs en trois classes, en utilisant 4 caractéristiques. Pour simplification, on va utiliser seulement 2 caractéristiques: Petal Length (cm); Petal Width (cm). D'après [Ce tutoriel](https://teddykoker.com/2019/06/multi-class-classification-with-logistic-regression-in-python/) ces 2 caractéristiques sont suffisantes.\n",
    "\n",
    "**Dans cette partie, vous n'avez rien à programmer. Mais, il faut analyser les résultats à la fin**\n",
    "\n",
    "Deux solutions à analyser : \n",
    "- Entrainer 3 modèles de régression logistique binaire\n",
    "- Entrainer 1 modèle de régression logistique multinomial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "\n",
    "iris_data = iris.data\n",
    "\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On va garder seulement les informations \"petal\"\n",
    "Xiris = iris_data.iloc[:, 2:].values # Deux dernières colonnes\n",
    "\n",
    "Xiris[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0      0\n",
       " 1      0\n",
       " 2      0\n",
       " 3      0\n",
       " 4      0\n",
       "       ..\n",
       " 145    2\n",
       " 146    2\n",
       " 147    2\n",
       " 148    2\n",
       " 149    2\n",
       " Name: target, Length: 150, dtype: int64,\n",
       " array(['setosa', 'versicolor', 'virginica'], dtype='<U10'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_target = iris.target\n",
    "iris_target_names = iris.target_names\n",
    "\n",
    "iris_target, iris_target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yiris = iris_target.iloc[:].values # Les classes\n",
    "\n",
    "Yiris[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Séparabilité des classes\n",
    "\n",
    "Ici, on veut juger si les classes sont séparables visuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "couleurs = ['red', 'green', 'blue']\n",
    "for i in range(len(iris_target_names)):\n",
    "    idx = Yiris == i\n",
    "    plt.scatter(Xiris[idx, 0], Xiris[idx, 1], color=couleurs[i], label=iris_target_names[i])\n",
    "    \n",
    "plt.xlabel('sepal_length')\n",
    "plt.ylabel('sepal_width')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les résultats**\n",
    "\n",
    "- Que remarquez-vous concernant la séparabilité des 3 classes?\n",
    "- Donner une hypothèse concernant la performance des modèles sur ce dataset (Rappel, Précision)\n",
    "- Justifier cette hypothèse (Rappel, Précision) en comparant les 3 classes\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. One-vs-Rest OU One-vs-One\n",
    "\n",
    "Nous avons entrainé deux modèles : \n",
    "\n",
    "- **One-vs-Rest** : ici, trois sous-modèles binaires sont entraînés ; un pour chaque class. Chaque sous modèle détecte si l'échantillon appartient à sa classe ou non. Lors de la prédiction, on prend la classe avec le max de probabilité\n",
    "- **One-vs-One** : ici, un modèle de régression logistique multinomiale (maximum entropy) est entraîné pour séparer les trois classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "Xiris_train, Xiris_test, Yiris_train, Yiris_test = train_test_split(Xiris, Yiris, test_size=0.2, random_state=0)  \n",
    "\n",
    "len(Xiris_train), len(Xiris_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-Rest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.98      0.94      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "One-vs-One\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "one2rest = LogisticRegression(solver=\"lbfgs\", penalty=\"none\", multi_class=\"ovr\")\n",
    "one2rest.fit(Xiris_train, Yiris_train)\n",
    "\n",
    "one2one = LogisticRegression(solver=\"lbfgs\", penalty=\"none\", multi_class=\"multinomial\")\n",
    "one2one.fit(Xiris_train, Yiris_train)\n",
    "\n",
    "print(\"One-vs-Rest\")\n",
    "print(classification_report(Yiris_test, one2rest.predict(Xiris_test)))\n",
    "\n",
    "print(\"One-vs-One\")\n",
    "print(classification_report(Yiris_test, one2one.predict(Xiris_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les résultats**\n",
    "\n",
    "On remarque que la performance de One-vs-One est meilleure que celle de One-vs-Rest\n",
    "\n",
    "- Pourquoi ? (en se basant sur la limite de décision et les paramètres)\n",
    "- Quel mécanisme de ces deux (One-vs-One, One-vs-Rest) est affecté beaucoup plus par les valeurs aberrantes (les échantillons d'une classe qui peuvent se retrouver aux milieu d'une autre classe)\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
